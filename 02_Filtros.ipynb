{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6cb2a4b",
   "metadata": {},
   "source": [
    "# Filtrar detecciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68b31aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de especies: 631\n",
      "Número de proyectos: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import birdnames as bn\n",
    "from pathlib import Path\n",
    "\n",
    "# Leer todos las detecciones para luego fusionarlas\n",
    "folder_path = \"./Detecciones_estandarizadas\"\n",
    "\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "dfs = [pd.read_csv(os.path.join(folder_path, f)) for f in csv_files]\n",
    "df_concat = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Número de especies y proyectos\n",
    "print(f\"Número de especies: {df_concat['speciesName'].nunique()}\")\n",
    "print(f\"Número de proyectos: {df_concat['projectName'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec983edd",
   "metadata": {},
   "source": [
    "## Filtrar especies objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1ac18bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speciesName\n",
       "Nothocrax urumutum          11874\n",
       "Myrmothera campanisona       4002\n",
       "Cyanocorax violaceus         3762\n",
       "Liosceles thoracicus         2104\n",
       "Ramphastos tucanus           1951\n",
       "Piaya melanogaster           1852\n",
       "Percnostola rufifrons         849\n",
       "Hylophylax naevius            426\n",
       "Akletos melanoceps            367\n",
       "Thamnomanes caesius           144\n",
       "Chaetura pelagica             110\n",
       "Corythopis torquatus          101\n",
       "Phlegopsis nigromaculata       85\n",
       "Pandion haliaetus              76\n",
       "Crax alector                   71\n",
       "Formicarius colma              63\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_df = pd.read_csv('./Especies_objetivo_escuchaton.csv')\n",
    "spp_putumayo = pd.read_csv('./Especies_objetivo_putumayo.csv')\n",
    "\n",
    "# Solo para putumayo (datos IAvH)\n",
    "targets_merged = pd.concat([targets_df, spp_putumayo], ignore_index=True)\n",
    "\n",
    "df_concat = df_concat[df_concat['speciesName'].isin(targets_merged ['Clements2024_Scientific_Name'])]\n",
    "\n",
    "# Numero de clips por especie con al menos 10 clips\n",
    "df_concat = df_concat.groupby('speciesName').filter(lambda x: len(x) >= 50)\n",
    "df_concat['speciesName'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22218c78",
   "metadata": {},
   "source": [
    "## Filtar por puntaje de deteccion (confidence)\n",
    "\n",
    "En este paso se van a seleccionar los clips para verificacion siguiendo el protocolo de Navine et al (2024) de 4 grupos (bins) logaritmicos, en la cual los puntajes de deteccion se van a convertir en logit y se seleccionaran un numero igual de grabaciones en 4 grupos de acuerdo a estos criterios:\n",
    "\n",
    "- El 50% de los puntajes mas bajos se asignaran al grupo 1\n",
    "- El 25% de los siguientes puntajes mas bajos de asignaran al grupo 2\n",
    "- El 12,5 % de los siguientes puntajes mas bajos de asignaran al grupo 3\n",
    "- El 12,5 % de los siguientes puntajes mas bajos de asignaran al grupo 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecbbb796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def safe_logit(p, eps=1e-6):\n",
    "    # Convert to numpy array\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    # Apply epsilon adjustment\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    # Compute logit\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "quantiles = [0, 0.50, 0.75, 0.875, 1.0]\n",
    "labels = [1, 2, 3, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cef600d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_additional_selection_table(csv):\n",
    "        table=csv[['speciesName','confidence','segmentID','filePath','startTime','endTime','classifiedBy','timestamp','recorderID','startTime','endTime','Begin Path']]\n",
    "        raven = pd.DataFrame()\n",
    "        raven = table.sort_values(by='confidence', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        raven['Selection'] = range(1, len(raven) + 1)\n",
    "        raven['View'] = 'Spectrogram 1'\n",
    "        raven['Channel'] = 1\n",
    "        raven['groupID'] = raven['segmentID'].astype(str).str.split('_').str[0]\n",
    "        raven['Begin Time (s)'] = 0\n",
    "        raven['End Time (s)'] = 3\n",
    "        raven['File Offset (s)'] = 0\n",
    "        raven = raven.rename(columns={'speciesName': 'Scientific Name'})\n",
    "\n",
    "        # Convert to scientific names\n",
    "        converter = bn.Converter(\n",
    "            from_type=\"scientific_name\",\n",
    "            to_type=\"common_name\",\n",
    "            from_authority=\"ebird\"\n",
    "        )\n",
    "        raven['Common Name'] = converter.convert(raven['Scientific Name'])\n",
    "        \n",
    "        #raven = raven.rename(columns={'segmentID': 'Begin Path'})\n",
    "        #raven['Begin Path'] = raven['filePath'].str.split('/').str[-1]\n",
    "        \n",
    "        raven = raven.rename(columns={'filePath': ' Orig_path'})\n",
    "        raven['verify'] = 'NA'\n",
    "        raven['song_type'] = 'NA'\n",
    "        raven['reference'] = 'NA'\n",
    "        raven['add2library'] = 0\n",
    "        raven['notes'] = 'NA'\n",
    "        raven['Method'] = raven['classifiedBy'].apply(lambda x: 'BirdNET' if ('birdnet' in str(x).lower()) else (\"Perch\" if ('perch' in str(x).lower()) else None))\n",
    "        raven['Date'] = raven['timestamp'].apply(lambda x: x[:10] if pd.notna(x) else None)\n",
    "        raven = raven.rename(columns={'recorderID': 'Punto'})\n",
    "        raven = raven.rename(columns={'startTime': 'Orig_start'})\n",
    "        raven = raven.rename(columns={'endTime': 'Orig_end'})\n",
    "        raven = raven.loc[:, ~raven.columns.duplicated()]\n",
    "\n",
    "        order= ['Selection', 'View', 'Channel', 'group_id', 'Begin Time (s)',\n",
    "        'End Time (s)', 'File Offset (s)', 'Common Name', 'Scientific Name',\n",
    "        'Confidence', 'Scientific Name2', 'Scientific Name3', 'Date', 'Punto',\n",
    "        'Begin Path', 'Orig_path', 'Orig_start', 'Orig_end', 'verify',\n",
    "        'song_type', 'reference', 'add2library', 'notes', 'method']\n",
    "\n",
    "        # Sort the columns of the excel DataFrame based on the 'order' list, keeping only those columns that exist in excel\n",
    "        existing_columns = [col for col in order if col in raven.columns]\n",
    "        raven = raven.reindex(columns=existing_columns)\n",
    "        return raven\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6584168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selections(df, clips_por_grupo):\n",
    "    for species, group in df.groupby('speciesName'):\n",
    "        group = group.copy()\n",
    "        # Use the safe_logit function for logits\n",
    "        group['logits'] = safe_logit(group['confidence'])\n",
    "        group[\"logit_group\"] = pd.qcut(group[\"logits\"], q=quantiles, labels=labels, duplicates=\"drop\" )\n",
    "   \n",
    "        logit_group_order = [4, 3, 2, 1]  # from highest to lowest group\n",
    "        target_per_group = clips_por_grupo\n",
    "        sampled_rows_1 = []\n",
    "        sampled_rows_2 = []\n",
    "\n",
    "        extra_needed_1 = 0\n",
    "        extra_needed_2 = 0\n",
    "\n",
    "        # Loop over groups from highest to lowest\n",
    "        for current_group in logit_group_order:\n",
    "            current_rows = group[group['logit_group'] == current_group]\n",
    "            n_rows = len(current_rows)\n",
    "\n",
    "            # FIRST subset (as before, random_state=42)\n",
    "            if n_rows >= (target_per_group + extra_needed_1):\n",
    "                sampled_current_1 = current_rows.sample(n=target_per_group + extra_needed_1, random_state=42)\n",
    "                sampled_rows_1.append(sampled_current_1)\n",
    "                extra_needed_1 = 0\n",
    "                # For the 2nd subset, use the remaining (exclude those from sample 1)\n",
    "                remaining_rows = current_rows.drop(sampled_current_1.index)\n",
    "                available_for_2 = len(remaining_rows)\n",
    "                need_2 = target_per_group + extra_needed_2\n",
    "                if available_for_2 >= need_2:\n",
    "                    sampled_current_2 = remaining_rows.sample(n=need_2, random_state=99)\n",
    "                    sampled_rows_2.append(sampled_current_2)\n",
    "                    extra_needed_2 = 0\n",
    "                else:\n",
    "                    sampled_current_2 = remaining_rows\n",
    "                    sampled_rows_2.append(sampled_current_2)\n",
    "                    extra_needed_2 = need_2 - available_for_2\n",
    "            else:\n",
    "                # If not enough for subset 1, just assign all, so subset 2 has nothing from this bin\n",
    "                sampled_current_1 = current_rows\n",
    "                sampled_rows_1.append(sampled_current_1)\n",
    "                extra_needed_1 = (target_per_group + extra_needed_1) - n_rows\n",
    "                # For subset 2, nothing left\n",
    "                sampled_current_2 = current_rows.iloc[0:0]\n",
    "                sampled_rows_2.append(sampled_current_2)\n",
    "                extra_needed_2 += (target_per_group)  # Not enough for the second subset either\n",
    "\n",
    "        # Concatenate selected rows\n",
    "        sampled_1 = pd.concat(sampled_rows_1, ignore_index=True)\n",
    "        sampled_2 = pd.concat(sampled_rows_2, ignore_index=True)\n",
    "        output_folder = \"./Detecciones_filtradas\"\n",
    "        output_additional = \"./Detecciones_filtradas_adicionales\"\n",
    "        species_folder = os.path.join(output_folder)\n",
    "        additional_folder = os.path.join(output_additional)\n",
    "        os.makedirs(species_folder, exist_ok=True)\n",
    "        os.makedirs(additional_folder, exist_ok=True)\n",
    "        del sampled_1['logits']\n",
    "        del sampled_2['logits']\n",
    "        output_path_1 = os.path.join(species_folder, f\"{species}.csv\")\n",
    "        output_path_2 = os.path.join(additional_folder, f\"{species}_adicionales.csv\")\n",
    "        sampled_1.to_csv(output_path_1, index=False)\n",
    "        sampled_2.to_csv(output_path_2, index=False)\n",
    "        raven_1 = get_additional_selection_table(csv=sampled_1)\n",
    "        raven_2 = get_additional_selection_table(csv=sampled_2)\n",
    "        dir_tables = f\"./Datos/{species}\"\n",
    "        dir_tables_additional = f\"./Datos_adicionales/{species}\"\n",
    "        os.makedirs(dir_tables, exist_ok=True)\n",
    "        os.makedirs(dir_tables_additional, exist_ok=True)\n",
    "        output_path_raven_1 = os.path.join(dir_tables, f\"{species}.csv\")\n",
    "        output_path_raven_2 = os.path.join(dir_tables_additional, f\"{species}_adicionales.csv\")\n",
    "        raven_1.to_csv(output_path_raven_1.replace('.csv','_S1.txt'), index=False, sep='\\t')\n",
    "        raven_2.to_csv(output_path_raven_2.replace('.csv','_S1.txt'), index=False, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c064a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_selections(df=df_concat, clips_por_grupo=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "song4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
