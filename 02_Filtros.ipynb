{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6cb2a4b",
   "metadata": {},
   "source": [
    "# Filtrar detecciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "283d1236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2061"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "begin_path = '/Users/santiagoruiz/Downloads/Validation_1to5'\n",
    "\n",
    "# Recursively list all .wav files in begin_path and subfolders\n",
    "wav_files = []\n",
    "for root, dirs, files in os.walk(begin_path):\n",
    "    for file in files:\n",
    "        if file.lower().endswith('.wav'):\n",
    "            wav_files.append(os.path.join(root, file))\n",
    "\n",
    "len(wav_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25507a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segmentID</th>\n",
       "      <th>Full_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [segmentID, Full_path]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('/Users/santiagoruiz/Documents/Escuchaton/Detecciones_estandarizadas/nambi_std_subset.csv')\n",
    "# Create a DataFrame with 'segmentID' and full path by matching with wav_files\n",
    "\n",
    "# Get the list of segmentIDs as a set for fast lookup/matching\n",
    "segment_ids = set(data['segmentID'])\n",
    "\n",
    "# Function to extract segmentID from a file path (without .wav extension)\n",
    "def extract_segment_id_from_path(path):\n",
    "    return os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "# Build a mapping from segmentID to full path\n",
    "id_to_path = {}\n",
    "for wav_path in wav_files:\n",
    "    seg_id = extract_segment_id_from_path(wav_path)\n",
    "    if seg_id in segment_ids:\n",
    "        id_to_path[seg_id] = wav_path\n",
    "\n",
    "# Build the output dataframe\n",
    "segment_df = pd.DataFrame({\n",
    "    'segmentID': [],\n",
    "    'Full_path': [],\n",
    "})\n",
    "\n",
    "for seg_id in data['segmentID']:\n",
    "    full_path = id_to_path.get(seg_id, None)\n",
    "    segment_df = pd.concat([\n",
    "        segment_df,\n",
    "        pd.DataFrame({'segmentID': [seg_id], 'Full_path': [full_path]})\n",
    "    ], ignore_index=True)\n",
    "\n",
    "segment_df[segment_df['Full_path'].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68b31aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de especies: 32\n",
      "Número de proyectos: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import birdnames as bn\n",
    "from pathlib import Path\n",
    "\n",
    "# Leer todos las detecciones para luego fusionarlas\n",
    "folder_path = \"./Detecciones_estandarizadas\"\n",
    "begin_path='/Users/santiagoruiz/Downloads/Validation_1to5'\n",
    "\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "dfs = [pd.read_csv(os.path.join(folder_path, f)) for f in csv_files]\n",
    "df_concat = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Número de especies y proyectos\n",
    "print(f\"Número de especies: {df_concat['speciesName'].nunique()}\")\n",
    "print(f\"Número de proyectos: {df_concat['projectName'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec983edd",
   "metadata": {},
   "source": [
    "## Filtrar especies objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1ac18bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speciesName\n",
       "Grallaria alleni              28904\n",
       "Cephalopterus penduliger      20783\n",
       "Hypopyrrhus pyrohypogaster      926\n",
       "Glaucidium nubicola             806\n",
       "Henicorhina negreti             586\n",
       "Vireo masteri                   430\n",
       "Grallaria rufocinerea           420\n",
       "Pyroderus scutatus              356\n",
       "Odontophorus melanonotus        223\n",
       "Galbula pastazae                163\n",
       "Dysithamnus occidentalis        151\n",
       "Sericossypha albocristata       132\n",
       "Ammodramus savannarum            82\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_df = pd.read_csv('./especies_objetivo_escuchaton.csv')\n",
    "df_concat = df_concat[df_concat['speciesName'].isin(targets_df['Clements2024_Scientific_Name'])]\n",
    "\n",
    "# Numero de clips por especie\n",
    "df_concat = df_concat.groupby('speciesName').filter(lambda x: len(x) >= 50)\n",
    "df_concat['speciesName'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22218c78",
   "metadata": {},
   "source": [
    "## Filtar por puntaje de deteccion (confidence)\n",
    "\n",
    "En este paso se van a seleccionar los clips para verificacion siguiendo el protocolo de Navine et al (2024) de 4 grupos (bins) logaritmicos, en la cual los puntajes de deteccion se van a convertir en logit y se seleccionaran un numero igual de grabaciones en 4 grupos de acuerdo a estos criterios:\n",
    "\n",
    "- El 50% de los puntajes mas bajos se asignaran al grupo 1\n",
    "- El 25% de los siguientes puntajes mas bajos de asignaran al grupo 2\n",
    "- El 12,5 % de los siguientes puntajes mas bajos de asignaran al grupo 3\n",
    "- El 12,5 % de los siguientes puntajes mas bajos de asignaran al grupo 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59806239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clips_por_grupo = 12\n",
    "\n",
    "\n",
    "def safe_logit(p, eps=1e-6):\n",
    "    # Convert to numpy array\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    # Apply epsilon adjustment\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    # Compute logit\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "quantiles = [0, 0.50, 0.75, 0.875, 1.0]\n",
    "labels = [1, 2, 3, 4]\n",
    "\n",
    "for species, group in df_concat.groupby('speciesName'):\n",
    "    group = group.copy()\n",
    "    # Use the safe_logit function for logits\n",
    "    group['logits'] = safe_logit(group['confidence'])\n",
    "    group[\"logit_group\"] = pd.qcut(group[\"logits\"], q=quantiles, labels=labels, duplicates=\"drop\" )\n",
    "    # INSERT_YOUR_CODE\n",
    "    # Rewritten selection according to specification\n",
    "    logit_group_order = [4, 3, 2, 1]  # from highest to lowest group\n",
    "    target_per_group = clips_por_grupo\n",
    "    sampled_rows = []\n",
    "\n",
    "    group_counts = group['logit_group'].value_counts().reindex(logit_group_order, fill_value=0)\n",
    "    extra_needed = 0\n",
    "\n",
    "    # Loop over groups from highest to lowest\n",
    "    for current_group in logit_group_order:\n",
    "        current_rows = group[group['logit_group'] == current_group]\n",
    "        n_rows = len(current_rows)\n",
    "\n",
    "        if n_rows >= (target_per_group + extra_needed):\n",
    "            sampled_current = current_rows.sample(n=target_per_group + extra_needed, random_state=42)\n",
    "            sampled_rows.append(sampled_current)\n",
    "            extra_needed = 0\n",
    "        else:\n",
    "            sampled_current = current_rows\n",
    "            sampled_rows.append(sampled_current)\n",
    "            extra_needed = (target_per_group + extra_needed) - n_rows\n",
    "\n",
    "    # Concatenate selected rows\n",
    "    sampled = pd.concat(sampled_rows, ignore_index=True)\n",
    "    output_folder = \"./Detecciones_filtradas\"\n",
    "    species_folder = os.path.join(output_folder)\n",
    "    os.makedirs(species_folder, exist_ok=True)\n",
    "    del sampled['logits']\n",
    "    output_path = os.path.join(species_folder, f\"{species}.csv\")\n",
    "    sampled.to_csv(output_path, index=False)\n",
    "\n",
    "    table=sampled\n",
    "    table=table[['speciesName','confidence','segmentID','filePath','startTime','endTime','classifiedBy','timestamp','recorderID','startTime','endTime']]\n",
    "    raven = pd.DataFrame()\n",
    "    raven = table.sort_values(by='confidence', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    raven['Selection'] = range(1, len(raven) + 1)\n",
    "    raven['View'] = 'Spectrogram 1'\n",
    "    raven['Channel'] = 1\n",
    "    raven['groupID'] = raven['segmentID'].astype(str).str.split('_').str[0]\n",
    "    raven['Begin Time (s)'] = 0\n",
    "    raven['End Time (s)'] = raven['classifiedBy'].apply(lambda x: 3 if ('birdnet' in str(x).lower()) else (5 if ('perch' in str(x).lower()) else None))\n",
    "    raven['File Offset (s)'] = 0\n",
    "    raven = raven.rename(columns={'speciesName': 'Scientific Name'})\n",
    "\n",
    "    # Convert to scientific names\n",
    "    converter = bn.Converter(\n",
    "        from_type=\"scientific_name\",\n",
    "        to_type=\"common_name\",\n",
    "        from_authority=\"ebird\"\n",
    "    )\n",
    "    raven['Common Name'] = converter.convert(raven['Scientific Name'])\n",
    "    \n",
    "\n",
    "    ############################################################################################################################\n",
    "    raven = raven.rename(columns={'segmentID': 'Begin Path'})\n",
    "\n",
    "    # Build list of all files (recursively) in the begin_path folder\n",
    "    all_files = [str(path) for path in Path(begin_path).rglob('*') if path.is_file()]\n",
    "\n",
    "    def find_full_path(snippet):\n",
    "        # Search for the first file path that contains the snippet\n",
    "        # (handles case when input is e.g. just filename without extension or prefix)\n",
    "        matches = [file for file in all_files if snippet in os.path.basename(file)]\n",
    "        return matches[0] if matches else None\n",
    "\n",
    "    raven['Begin Path'] = raven['Begin Path'].apply(find_full_path)\n",
    "    \n",
    "    ############################################################################################################################\n",
    "    raven = raven.rename(columns={'filePath': ' Orig_path'})\n",
    "    raven['verify'] = 'NA'\n",
    "    raven['song_type'] = 'NA'\n",
    "    raven['reference'] = 'NA'\n",
    "    raven['add2library'] = 0\n",
    "    raven['notes'] = 'NA'\n",
    "    raven['Method'] = raven['classifiedBy'].apply(lambda x: 'BirdNET' if ('birdnet' in str(x).lower()) else (\"Perch\" if ('perch' in str(x).lower()) else None))\n",
    "    raven['Date'] = raven['timestamp'].apply(lambda x: x[:10] if pd.notna(x) else None)\n",
    "    raven = raven.rename(columns={'recorderID': 'Punto'})\n",
    "    raven = raven.rename(columns={'startTime': 'Orig_start'})\n",
    "    raven = raven.rename(columns={'endTime': 'Orig_end'})\n",
    "    raven = raven.loc[:, ~raven.columns.duplicated()]\n",
    "\n",
    "    order= ['Selection', 'View', 'Channel', 'group_id', 'Begin Time (s)',\n",
    "       'End Time (s)', 'File Offset (s)', 'Common Name', 'Scientific Name',\n",
    "       'Confidence', 'Scientific Name2', 'Scientific Name3', 'Date', 'Punto',\n",
    "       'Begin Path', 'Orig_path', 'Orig_start', 'Orig_end', 'verify',\n",
    "       'song_type', 'reference', 'add2library', 'notes', 'method']\n",
    "\n",
    "    # Sort the columns of the excel DataFrame based on the 'order' list, keeping only those columns that exist in excel\n",
    "    existing_columns = [col for col in order if col in raven.columns]\n",
    "    raven = raven.reindex(columns=existing_columns)\n",
    "    output_raven='./Tablas_de_seleccion'\n",
    "\n",
    "    species_folder_raven = os.path.join(output_raven)\n",
    "    os.makedirs(species_folder_raven, exist_ok=True)\n",
    "\n",
    "    output_raven = os.path.join(species_folder_raven, f\"{species}.csv\")\n",
    "    raven.to_csv(output_raven.replace('.csv', '_merged.txt'), index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68326c48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "song4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
